\subsection{Wielogłowicowa głęboka rekurencyjna sieć neuronowa}
Ideą rekurencyjnych sieci neuronowych (ang. Recurent Neural Networks, RNN) jest wykorzystanie danych w postaci sekwencji. 
Sieć neuronowa typu feed forward przyjmuje za każdym razem stałą liczbę wejść, następnie na ich podstawie generuje wynik. 
Rekurencyjna sieć neuronowa natomiast nie konsumuje wszystkich danych wejściwych na raz. Zamiast tego,
dane pobierane są w krokach czasowych. W każdym kroku RNN wykonuje obliczenia i zwraca wyjście. 
To wyjście jest w następnym kroku łączone z kolejnym wejściem by zwrócić kolejne dane wyjściowe. 
Ten proces jest powtarzany dopóki model nie zostanie wyuczony. Możliwość korzystania z 
informacji kontekstowych z poprzednich danych wejściowych jest szczególną cechą sieci typu RNN. Dlatego
są one używane do problemów gdzie dane mają strukturę sekwencyjną, na przykład
przy analizie tekstów ale także sygnałów audio czy ruchów obiektów.

\begin{figure}[!ht]
\includegraphics[width=\linewidth]{./images/rnn.png}
\caption{Schemat sieci typu RNN.}
\label{fig:test3}
\end{figure}
Jak widać na rysunku 2, obliczenia dla każdego kroku czasowego są wykonywne z nowymi danymi wejściowymi
ale biorąc pod uwagę kontekst z poprzedniego kroku czasowego. 
\begin{wrapfigure}{r}{0.17\textwidth}
\vspace{-4mm}
\includegraphics[width=\linewidth]{./images/many-to-one.png}
\caption{Sieć typu ``many to one''.}
\label{fig:test3}
\vspace{-4mm}
\end{wrapfigure}
Funkcja odwzorowująca wejście oraz poprzedni kontekst na wyjście jest dokładnie tą samą przez cały proces
uczenia się. Neurony z połączeniami rekurencyjnymi działają jak pamięć - potencjalnie są w stanie modelować relacje
występujące z dowolnie długim odstępem czasowym. 
Rozwinięta w czasie sieć tworzy głęboką sieć neuronową 
ze wszystkimi tego konsekwencjami jak zanikający lub znacznie rzadziej wybuchający gradient. 
Ważną cechą tych sieci jest również fakt, że obliczenia dla długich sekwencji są bardzo kosztowne.
Sieci typu RNN generują bardzo dużo danych wyjściowych w procesie uczenia i to od nas zależy jak je 
wykorzystamy. Dla naszego problemu korzystamy z sieci typu ``many to one'', co oznacza, że korzystamy
z danych wyjściowych z ostatniego kroku czasowego.

\newpage
Rekurencyjną sieć neuronową definiujemy jako:

\begin{align}
  &h_t = f(x_t, h_{t-1}) = \text{tanh}(W^Th_{t-1} + U^Tx_t) \\ 
  &y_t = g(h_t) = \text{logSoftMax}(V^Th_t)
\end{align}

gdzie: \newline
$f$ - funkcja aktywacji, \newline
$g$ - funkcja aktywacji, \newline
$h_t$ - warstwa ukryta w czasie t, \newline
$y_t$ - wektor wynikowy w czasie t , \newline
$x_t$ - wektor wejściowy w czasie t, \newline
$W, U, V$ - macierze wag, \newline


\begin{figure}[H]
\centering
\includegraphics[height=7cm]{./images/multiheaded-rnn.png}
\caption{Schemat sieci wielogłowicowej użytej w naszej pracy - na rysunku widać wspólną część rekurencyjną zaznaczoną na zielono
oraz kolejne głowice oznaczone literami $y^{(k)}$ (gdzie k jest liczbą naturalną), które interpretują ostatni krok czasowy z sieci rekurencyjnej, każda
dla swojego autora, i predukują kolejny znak.}
\label{fig:test5}
\end{figure}

Termin ``wielogłowicowa'' odnosi się do wielu warstw liniowych, które na podstawie danych wyjściowych z 
sieci rekurencyjnej predykują rozkład prawdopodobieństwa następnego znaku. Każdy autor ma swoją głowicę, która 
szkolona jest tylko na jego tekstach.