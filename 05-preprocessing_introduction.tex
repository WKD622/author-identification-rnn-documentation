\newpage
\section{Preprocesing}

Preprocesing to proces przygotowywania danych wejściowych tak by nadawały się do szkolenia sieci.

\subsection{Teoria}

Proces preprocesingu dzieli się na trzy etapy.
\begin{enumerate}
	\item Redukcja alfabetu (mapowanie znaków)
	\item Zamiana zredukowanych tekstów na tensory
	\item Podział tensorów na batche
\end{enumerate}

Należy przy tym dodać że domyślnie dostępnymi językami w bibliotece są:
\languages 


\subsubsection{Redukcja alfabetu}
Redukcja alfabetu to sprytny sposób zmniejszenia liczby znaków w alfabecie, tak by uczenie sieci było
efektywniejsze. Możemy definiować nasze własne 'mappery'. Przykładowy fragment pliku do redukcji alfabetu języka
angielskiego znajduje się poniżej (został on znacznie skrócony wzlgędem oryginału żeby zachować lepszą
czytelność)

\begin{python}
charmap = {
    u' ': u' ',                 # kept Zs (92447)
    u'e': u'e',
    u't': u't',
    u'o': u'o',
    u'a': u'a',
    u'n': u'n',
    u'i': u'i',
    u's': u's',
    u'h': u'h',
    u'r': u'r',
    u'\n': u'\n',               # kept Cc (13082)
    u'l': u'l',
    u'd': u'd',
    u'u': u'u',
    u'y': u'y',
    u'm': u'm',
    u'g': u'g',
    u'w': u'w',
    u',': u',',                 # kept Po (5250)
    u"'": "'",                  # single quote
    u'.': u'.',                 # kept Po (4826)
    u'f': u'f',
    u'c': u'c',
    u'b': u'b',
    u'p': u'p',
    u'-': u'-',                 # kept Pd (3253)
    u'k': u'k',
    u'I': u'\xb9i',             # decomposed caps
    u'v': u'v',
}
\end{python}

Tak przygotowany 'mapper' pozwala na zamiane wybranych znaków, które nie są kluczowe w procesie 
szkolenia sieci na wybrane, inne znaki.

\newpage
\subsubsection{Zamiana tekstów na tensory}
Aby zamienić teksty na tensory potrzebny jest nam alfabet. Alfabet definiujemy jako listę, której 
elementy są znakami. Tak wygląda fragment alfabetu z języka angielskiego:
\begin{python}
alphabet = [
    ' ',
    'e',
    't',
    'o',
    'a',
    'n',
    'i',
    's',
    'h',
    'r',  # 10
    '\\n',
    'l',
    'd',
    'u',
    'y',
    'm',
    'g',
    'w',
    ',',
    "'",  # 20
    '.',
    'f',
    'c',
    'b',
]
\end{python}

Każdy znak zredukowanego pliku wejściowego zamieniany jest na wektor wykorzystując alfabet, 
a wektory 'umieszczane' są w tensorze. Wykorzystywane przy tym jest \texttt{one hot encoding}.
Zamianę liter na wektory najłatwiej pokazać na przykładzie.

Załóżmy że nasz alfabet wygląda następująco:

\begin{python}
alphabet = [
    'a',  #0
    'b',  #1
    'c',  #2
    'd',  #3
 ]
\end{python} 
Jeśli zamienimy literę 'a' na wektor otrzymamy: 
 
\vspace{2mm}
$
\begin{bmatrix} 
1, & 0, & 0, & 0
\end{bmatrix} 
$
\vspace{2mm}

Ponieważ litera 'a' jest na pozycji 0 w alfabecie liczba 1 pojawiła się na pozycji 0 w wektorze.
Długość wektora wynosi 4 ponieważ tyle znaków znajduje się w alfabecie. A więc dla litery 
'c' wektor wyglądałby w następujący sposób:

\vspace{2mm}
$
\begin{bmatrix} 
0, & 0, & 1, & 0
\end{bmatrix} 
$
\vspace{2mm}

W ten sposób konwertowane są wszystkie znaki w plikach, które następnie umieszczane są w tensorze.
Tensor jest tablicą jednowymiarową, której elementami są wyżej wspomniane wektory,
umieszczane tam zgodnie z kolejnością występowania liter w tekście. Następnie następuje podział na batche.

\subsubsection{Rozdzielenie tensorów na paczki}
Jednym ze sposobów przyspieszenia czasu uczenia jest podzielenie danych na paczki (ang. batche).
Pozwala nam to na wykonywanie obliczeń na wielu danych jednocześnie.

Załóżmy że mamy następujące wejście: ''abcdbdcaaadcb''
Zamieniamy litery na wektory wedle wcześniejszego opisu, gdzie:
\vspace{3mm}
\newline 
a =
$
\begin{bmatrix} 
1, & 0, & 0, & 0
\end{bmatrix} 
$
\vspace{3mm}
\newline
b = 
$
\begin{bmatrix} 
0, & 1, & 0, & 0
\end{bmatrix} 
$
\vspace{3mm}
\newline 
c =
$
\begin{bmatrix} 
0, & 0, & 1, & 0
\end{bmatrix} 
$
\vspace{3mm}
\newline 
d =
$
\begin{bmatrix} 
0, & 0, & 0, & 1
\end{bmatrix} 
$
\vspace{3mm}
\newline 

Wektory umieszczamy w tensorze. Tensor dzielimy na batche wielkości 3. Tensor po podziale na batche 
wyglądałby następująco:
\vspace{3mm}
\newline
$
\begin{bmatrix}
a & b & c\\
d & b & d\\
c & a & a\\
a & d & c
\end{bmatrix}
$
\vspace{3mm}
\newline 

Ostatnie b zostaje odrzucone, ponieważ nie tworzy pełnego wiersza.
Tak przygotowane zestawy danych możemy wykorzystać do szkolenia sieci.


\newpage
\subsection{Pliki wejściowe oraz wyjściowe}
Aby korzystać z modułu preprocessingu należy zachować określoną konwencję danych wejściowych. 

\subsubsection{Struktura wejścia}

\begin{enumerate}
	\item Wszystkie pliki tekstowe muszą być w formacie txt,
	\item folder z danymi wejściwoymi musi mieć konkretną strukture. W folderze każdego autora
		  powinny znajdować się jego teksty: 
			\begin{itemize}
				\item known$X$.txt gdzie $X$ jest numerem tekstu, tekstów known.txt może być wiele.
				\item unknown.txt
			\end{itemize}
\end{enumerate}

Przykład: 

\myspace
\dirtree{%
.1 data.
.2 authors.
.3 EN001.
.4 known01.txt.
.4 unknown.txt.
.3 EN002.
.4 known01.txt.
.4 known02.txt.
.4 known03.txt.
.4 unknown.txt.
.3 EN003.
.4 known01.txt.
.4 unknown.txt.
.3 EN004.
.4 known01.txt.
.4 known01.txt.
.4 unknown.txt.
.3 EN005.
.4 known01.txt.
.4 unknown.txt.
}
\myspace

\newpage
\subsubsection{Struktura wyjścia}

Wszystkie pliki wejściowe są zamieniane na tensory i dzielone na batche. Wszystkie pliki knownX.txt 
danego autora są wcześniej łączone w jeden plik, który później staje się tensorem. W ten sposób 
otrzymujemy następująco wyglądający folder wyjściowy, gdzie każdy plik EN$X$.pt (gdzie $X$ to numer)
 jest tensorem:

\myspace
\dirtree{%
.1 tensors.
.2 known.
.3 EN001.
.4 EN001.pt.
.3 EN002.
.4 EN002.pt.
.3 EN003.
.4 EN003.pt.
.3 EN004.
.4 EN004.pt.
.3 EN005.
.4 EN005.pt.
.2 unknown.
.3 EN001.
.4 EN001.pt.
.3 EN002.
.4 EN002.pt.
.3 EN003.
.4 EN003.pt.
.3 EN004.
.4 EN004.pt.
.3 EN005.
.4 EN005.pt.
}
\myspace
