\newpage
\section{Podręcznik użytkownika}

\subsection{Struktura plików systemu}

\myspace
\dirtree{% 
.1 helpers.
.2 files.
.3 files\_operations.py.
.3 json\_loader.py.
.3 name\_convention.py.
.1 preprocesing \DTcomment{moduł preprocesingu}.
.2 chars\_mapping \DTcomment{metody niskopoziome odpowiedzialne za redukcje alfabetów}.
.3 mappers.
.4 en\_mapper.py.
.4 es\_mapper.py.
.4 gr\_mapper.py.
.4 nl\_mapper.py.
.3 map.py.
.2 data\_structs	\DTcomment{klasy do trzymania tekstów w formie obiektów}.
.3 reduced\_authors.py.
.2 to\_tensor \DTcomment{metody niskopoziome odpowiedzialne za zamianę tekstu na tensory}.
.3 alphabets.
.4 en\_alphabet.py.
.4 es\_alphabet.py.
.4 gr\_alphabet.py.
.4 nl\_alphabet.py.
.3 convert.py.
.2 constants.py.
.2 exceptions.py.
.2 preprocesing.py \DTcomment{główny plik modułu preprocesingu}.
.1 network \DTcomment{moduł treningu}.
.2 batch\_processing \DTcomment{moduł batchowania}.
.3 batching.py.
.3 evaluation\_batches.py.
.2 model.py \DTcomment{moduł sieci}.
.2 train.py. 
}
\myspace

\subsection{Wymagania}
\begin{itemize}
    \item Python 3.7
	\item torch 1.1.0.post2
    \item matplotlib 3.1.1
    \item numpy 1.17.0
 \end{itemize}

\subsection{Wykorzystanie modułu preprocesingu}

Aby korzystać z modułu preprocesingu należy zachować określoną konwencję danych wejściowych. 

\subsubsection{Struktura wejścia}

\begin{enumerate}
	\item Wszystkie pliki tekstowe muszą być w formacie txt,
	\item folder z danymi wejściowymi musi mieć konkretną strukturę. W folderze każdego autora
		  powinny znajdować się jego teksty: 
			\begin{itemize}
				\item known$X$.txt gdzie $X$ jest numerem tekstu, tekstów known.txt może być wiele.
				\item unknown.txt
			\end{itemize}
\end{enumerate}

Przykład: 

\myspace
\dirtree{%
.1 data.
.2 authors.
.3 EN001.
.4 known01.txt.
.4 unknown.txt.
.3 EN002.
.4 known01.txt.
.4 known02.txt.
.4 known03.txt.
.4 unknown.txt.
.3 EN003.
.4 known01.txt.
.4 unknown.txt.
.3 EN004.
.4 known01.txt.
.4 known01.txt.
.4 unknown.txt.
.3 EN005.
.4 known01.txt.
.4 unknown.txt.
}
\myspace

\newpage
\subsubsection{Struktura wyjścia}

Wszystkie pliki wejściowe są zamieniane na tensory i dzielone na batche. Wszystkie pliki knownX.txt 
danego autora są wcześniej łączone w jeden plik, który później staje się tensorem. W ten sposób 
otrzymujemy następująco wyglądający folder wyjściowy, gdzie każdy plik EN$X$.pt (gdzie $X$ to numer)
 jest tensorem:

\myspace
\dirtree{%
.1 tensors.
.2 known.
.3 EN001.
.4 EN001.pt.
.3 EN002.
.4 EN002.pt.
.3 EN003.
.4 EN003.pt.
.3 EN004.
.4 EN004.pt.
.3 EN005.
.4 EN005.pt.
.2 unknown.
.3 EN001.
.4 EN001.pt.
.3 EN002.
.4 EN002.pt.
.3 EN003.
.4 EN003.pt.
.3 EN004.
.4 EN004.pt.
.3 EN005.
.4 EN005.pt.
}
\myspace


Moduł preprocesingu został stworzony z myślą o prostej i intuicyjnej obsłudze. Wykorzystane przy
tym zostały Pythone'owe słowniki które pozwalają na uproszczenie sygnatury konstruktora obiektu do preprocesingu
o parametry, które w danym momencie nie są wykorzystywane. Najprościej zasadę działania modułu preprocesingu można 
pokazać na przykładach. Folder wejściowy z danymi musi być o identycznej strukturze jak ten na 
konkursie PAN.
Załóżmy że nasz katalog projektu wygląda w następujący sposób:


\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 authors.
.3 EN001.
.4 known01.txt.
.4 unknown.txt.
.3 EN002.
.4 known01.txt.
.4 known02.txt.
.4 known03.txt.
.4 unknown.txt.
.3 EN003.
.4 known01.txt.
.4 unknown.txt.
.3 EN004.
.4 known01.txt.
.4 known01.txt.
.4 unknown.txt.
.3 EN005.
.4 known01.txt.
.4 unknown.txt.
}
\myspace

Kiedy posiadamy już odpowiednią strukture katalogów z plikami wejściowymi możemy stworzyć 
odpowiedni obiekt i przystąpić do preprocesingu. Zakładamy że wszystkie linijki przedstawianego 
tutaj kodu znajdują się w widocznym na diagramie struktury folderów pliku \texttt{main.py}.

\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing()

\end{python}

Parametry konstruktora modułu preprocesingu będą określać w jaki sposób ma funkcjonować. 
Oto wszystkie możliwe parametry:
\begin{itemize}
	\item language
	\item data\_path
	\item tensors\_path
	\item mapped\_save\_path
	\item mapped\_source\_path
	\item batch\_size
	\item log\_lvl
	\item only\_reduce
	\item check\_files
	\item alphabet
	\item mapper
\end{itemize}

Parametry możemy używać w dowolnej kombinacji, jednak nie wszystkie z nich będą miały sens. Zbędne
parametry będą ignorowane.

\subsubsection{parametr language}
Parametr \texttt{language} jest parametrem obowiązkowym i pozwala nam określić język tekstów wejściowych.
\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en")

\end{python}

Jeśli go nie zdefiniujemy, powinniśmy otrzymać następujący błąd:

\begin{consolerror}
You have to specify language | EXAMPLE: (language="en")
\end{consolerror}

Domyślnie dostępnymi językami w bibliotece są:

\languages 

\subsubsection{parametr data\_path}
Parametr \texttt{data\_path} pozwala nam zdefiniować ścieżkę do danych wejściowych. W naszym przypadku
będzie ona wyglądała w następujący sposób:
\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en",
              data_path="../data/authors/")
                   
\end{python}

Jeśli jej nie zdefiniujemy, powinniśmy otrzymać następujący błąd:

\begin{consolerror}
You have to specify data source | EXAMPLE: (data_path=/some/path)
\end{consolerror}

Przy tak zdefiniowanych parametrach wszystkie operacje zostaną wykonane w pamięci RAM, a wyście w postaci
tensorów zostanie zapisane w katalogu pliku, który wykonujemy. Katalog z tensorami nosi nazwę \texttt{tensors}.
Struktura katalogów po wykonaniu będzie wyglądać następująco:

\myspace
\dirtree{%
.1 src.
.2 main.py.
.2 tensors \DTcomment{katalog wyjściowy z tensorami}.
.1 data.
.2 authors.
}
\myspace

\newpage
\subsubsection{parametr tensors\_path}
Jeśli chcemy zdefiniować ścieżkę zapisu tensorów możemy tego dokonać używając parametru 
\texttt{tensors\_path}. Dla przykładu zdefiniuję ją w następujący sposób. 
\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en",
              data_path="../data/authors/",
              tensors_path="../data/tensors/")
                   
\end{python}

Po wykonaniu pliku \texttt{main.py} otrzymamy następującą strukture katalogów:
\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 authors.
.2 tensors \DTcomment{katalog wyjściowy z tensorami}.
}
\myspace

Wszystkie operacje wykonywane są w tym przypadku przy użyciu pamięci ram.


\subsubsection{parametr mapped\_save\_path}
Mamy równiez możliwość zapisania plików o zredukowanym alfabecie jeśli chcielibyśmy je przejrzeć lub
wykorzystać w przyszłości ponownie. Do tego służy opisywany właśnie parametr \texttt{mapped\_save\_path}.
UWAGA! Za każdym razem kiedy korzystamy z tego argumentu wszystkie pliki spod wskazanej ścieżki są 
usuwane.

\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en",
              data_path="../data/authors/",
              tensors_path="../data/tensors/"
              mapped_save_path="../data/reduced_authors/")
                   
\end{python}
Struktura katalogu ze zredukowanymi tekstami jest identyczna jak struktura katalogu wejściowego. 

Po wykonaniu pliku \texttt{main.py} otrzymamy następującą strukture katalogów:

\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 authors.
.2 tensors \DTcomment{katalog wyjściowy z tensorami}.
.2 reduced\_authors \DTcomment{katalog ze zredukowanymi tekstami}.
}
\myspace

Wszystkie operacje wykonywane są w tym przypadku przy użyciu pamięci RAM.

\newpage
\subsubsection{parametr mapped\_source\_path}
Jeśli chcielibyśmy skorzystać ze zredukowanych wcześniej tekstów, które mamy zapisane na dysku, 
możemy w tym celu użyć parametru \texttt{mapped\_source\_path}.

Załóżmy przy tym następujące drzewo katalogów:
\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 reduced\_authors \DTcomment{katalog ze zredukowanymi tekstami}.
}
\myspace

\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en",
              tensors_path="../data/tensors/",
              mapped_source_path="../data/reduced_authors/")
                   
\end{python}

Po wykonaniu pliku \texttt{main.py} wyjściowe drzewo katalogów powinno wyglądać w następujący sposób:
\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 tensors \DTcomment{katalog wyjściowy z tensorami}.
.2 reduced\_authors \DTcomment{katalog ze zredukowanymi tekstami}.
}
\myspace

Pliki zredukowane zostały w tym przypadku odczytane z dysku, a nie z pamięci RAM jak miało to miejsce
we wcześniejszych przykładach. 
\newline

Możemy jednak tworzyć jeszcze bardziej złożone kombinacje perametrów. Załóżmy że nie chcemy aby pliki redukowane były
do pamięci RAM, a następnie w ten sposób przekazywane do tworzenia tensorów. Chcemy aby pliki najpierw
zredukowały się na dysk, a następnie z dysku zostały zamienione na tensory. 
Załóżmy wejściowe drzewo katalogów:

\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 authors.
}
\myspace

Kod wyglądałby następująco:
\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en",
              data_path="../data/authors/",
              tensors_path="../data/tensors/",
              mapped_save_path="../data/reduced_authors/",
              mapped_source_path="../data/reduced_authors/")

\end{python}

Parametr \texttt{data\_path} ma wyższy priorytet niż parametr \texttt{mapped\_source\_path}, a więc biblioteka najpierw 
pobierze niezredukowanych autorów z katalogu authors, następnie zredukuje ich do katalogu \texttt{reduced\_authors},
następnie wykorzystując katalog \texttt{reduced\_authors} stworzy tensory do katalogu tensors.

Wyjściowe drzewo katalogów będzie wyglądać następująco:

\myspace
\dirtree{%
.1 src.
.2 main.py.
.1 data.
.2 authors.
.2 reduced\_authors \DTcomment{katalog ze zredukowanymi tekstami}.
.2 tensors \DTcomment{katalog wyjściowy z tensorami}.
}

\myspace

\subsubsection{parametr only\_reduce}
Możemy także skorzystać tylko z redukcji, bez tworzenia tensorów. Służy do tego argument \texttt{only\_reduce}.
Aby nastąpiła jedynie redukcja alfabetu należy ustawić go na \texttt{True} jak w kodzie poniżej:

\begin{python}
from library.preprocesing.preprocesing import preprocesing

preprocesing(language="en",
              data_path="../data/authors/",
              mapped_save_path = "../data/reduced_authors/",
              reduce_only=True) 

\end{python}

\subsubsection{parametr batch\_size}
Możemy również zdefiniować rozmiar paczek na jakie podzielony zostanie nasz tensor. Dokonać tego możemy poprzez
parametr \texttt{batch\_size}. Domyślna wartość tego parametru to 20.

\begin{python}
preprocesing(language="en",
              data_path="../data/authors/",
              tensors_path="../data/tensors/",
              mapped_save_path="../data/reduced_authors/",
              batch_size=30)

\end{python}


W takim wypadku dane zostaną podzielone na paczki o rozmiarze 30.


