\newpage
\subsection{Algorytm optymalizacji Adam}
Adam (skrót od Adaptive Moment Estimation) jest obecnie zalecany do zadań optymalizacyjnych związanych z uczeniem maszynowym.
W tym algorytmie wykorzystywane są zarówno średnie bieżące gradientów jak i drugich momentów gradientów.
Zakładając parametry $\theta^{t}$ oraz funkcję straty $L^{(t)}$, gdzie $t$ jest aktualną iteracją treningu, 
aktualizacja parametrów zachodzi w następujący sposób:

\begin{align*}
  &m_w^{(t+1)} \leftarrow \beta_{1}m_w^{(t)} + (1 - \beta_1)\nabla_wL^{(t)} \\
  &v_w^{(t+1)} \leftarrow \beta_{2}v_w^{(t)} + (1 - \beta_2)(\nabla_wL^{(t)})^2 \\
  &\hat m_w = \frac{m_w^{(t+1)}}{1 - (\beta_1)^{t+1}} \\
  &\hat v_w = \frac{v_w^{(t+1)}}{1 - (\beta_2)^{t+1}} \\
  &w^{t+1} \leftarrow w^{(t)} -  \eta \frac{\hat m_w}{\sqrt{\hat v_w} + \epsilon} \\
\end{align*}

gdzie $\epsilon$ jest małym skalarem który ma zapobiec dzieleniu przez $0$, 
oraz $\beta _{1}$ i $\beta_{2}$, które są zapominającymi czynnikami odpowiednio dla gradientu oraz dla 
drugich momentów gradientu.
